
ğŸ¯ CIFAR-10 CNN Comparison Project


ğŸ“‹ Project Overview

This project implements and compares three distinct Convolutional Neural Network (CNN) architectures for image classification on the CIFAR-10 dataset. Through systematic experimentation and analysis, we investigate how different architectural choices impact classification performance, generalization ability, and training efficiency.

ğŸ” Research Question

*What CNN architecture provides the optimal balance of accuracy, efficiency, and generalization for CIFAR-10 image classification?*

ğŸ“Š Results Summary

Model	Test Accuracy	Key Features	Performance Insight
Deeper CNN ğŸ¥‡	77.62%	Batch Normalization, Global Avg Pooling, 3 Conv Blocks	Highest accuracy, best generalization
Baseline CNN ğŸ¥ˆ	77.02%	Simple 2-block architecture, dropout	Surprisingly effective, fastest training
CNN with Augmentation ğŸ¥‰	71.10%	Built-in data augmentation, extra conv layer	Needs careful tuning, generalization focus

ğŸ—ï¸ Models Architecture

1. Baseline CNN
text
Input â†’ [Conv(32) â†’ Conv(32) â†’ MaxPool â†’ Dropout] Ã—2 â†’ Flatten â†’ Dense(512) â†’ Softmax
Parameters: ~1.2M

Purpose: Performance baseline

2. CNN with Data Augmentation
text
Input â†’ RandomFlip â†’ RandomRotation â†’ [Conv Blocks] â†’ Classifier
Parameters: ~1.8M

Purpose: Test generalization via augmentation

3. Deeper CNN
   
text
Input â†’ [Conv â†’ BatchNorm â†’ Conv â†’ MaxPool â†’ Dropout] Ã—3 â†’ GlobalAvgPool â†’ Dense â†’ Softmax
Parameters: ~2.5M

Purpose: Explore depth benefits with stabilization


GPU recommended for faster training (but not required)

Installation
Clone the repository

bash:

git clone https://github.com/yourusername/cifar10-cnn-comparison.git
cd cifar10-cnn-comparison
Install dependencies

bash:

pip install -r requirements.txt
Run the notebook

bash:

jupyter notebook main.ipynb
Run in Google Colab
https://colab.research.google.com/assets/colab-badge.svg


ğŸ’¡ Key Insights

âœ… What Worked Well

Simple is effective: Baseline CNN achieved 77.02% with minimal complexity

Depth helps moderately: +0.6% improvement with deeper architecture

Batch normalization: Enables stable training of deeper networks

Global Average Pooling: Parameter-efficient alternative to flattening

âš ï¸ Challenges & Learnings

Data augmentation requires care: Aggressive augmentation hurt performance

Diminishing returns: Depth improvements were marginal on CIFAR-10

Validation-test gap: Some models generalized better than others

Animal classification hardest: Semantic similarity causes confusion

ğŸ“ Educational Value

This project is perfect for:

Students learning CNN architecture design

Researchers benchmarking on CIFAR-10

Developers starting image classification projects

Educators teaching deep learning concepts

Skills Learned:

CNN architecture design and implementation

Systematic model comparison methodology

Training visualization and analysis

Hyperparameter experimentation

Error analysis and interpretation

ğŸ”® Future Work

Planned Improvements:

Architecture Extensions:

ResNet with skip connections

EfficientNet compound scaling

Attention mechanisms

Training Enhancements:

Learning rate scheduling

Hyperparameter optimization

Ensemble methods

Dataset Expansion:

Test on CIFAR-100

Try Tiny ImageNet

Domain adaptation experiments

ğŸ“š References
Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images

He, K. et al. (2016). Deep Residual Learning for Image Recognition

Ioffe, S. & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training

Simard, P. et al. (2003). Best Practices for Convolutional Neural Networks

ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

Fork the repository

Create a feature branch (git checkout -b feature/AmazingFeature)

Commit your changes (git commit -m 'Add some AmazingFeature')

Push to the branch (git push origin feature/AmazingFeature)

Open a Pull Request

Areas for Contribution:

New CNN architectures

Advanced data augmentation techniques

Performance optimization

Additional visualizations

Documentation improvements


ğŸ™ Acknowledgments

CIFAR-10 Dataset Creators: Alex Krizhevsky, Vinod Nair, Geoffrey Hinton

TensorFlow/Keras Team for the excellent deep learning framework

Google Colab for providing free GPU resources

Open-source community for invaluable tools and libraries

ğŸ“ Contact
TAVONGA DUTUMA - tavongadutumah@gmail.com 


â­ Support

If you find this project useful, please consider giving it a star! â­

Why star this repo?

ğŸ“š Educational resource for learning CNN design

ğŸ”§ Ready-to-use code for your own projects

ğŸ“Š Clear visualizations for presentations

ğŸ¯ Practical insights from real experiments
